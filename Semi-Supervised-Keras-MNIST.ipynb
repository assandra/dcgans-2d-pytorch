{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Semi-Supervised GAN (SGAN) is a Generative Adversarial Network whose Discriminator is a multiclass classifier. Instead of distinguishing between two classes (real and fake), it learns to distinguish between N + 1 classes, where N is the number of classes in the training dataset, with one added for the fake examples produced by the Generator.\n",
    "\n",
    "The Discriminator thus needs to be changed from a binary classifier to a multiclass classifier.\n",
    "\n",
    "## Architecture\n",
    "![alt text](07_02.png \"SGAN Architecture\")\n",
    "\n",
    "The SGAN architecture is the same as the original GAN: it takes in a vector of random numbers and produces fake examples whose goal is to be indistinguishable from the training dataset - no change here.\n",
    "\n",
    "The SGAN Discriminator, however, diverges considerably from the original GAN implementation. Instead of two, it receives three kinds of inputs: fake examples produced by the Generator (x*), real examples without the labels from the training dataset (x), and the real examples with labels from the training dataset (x,y), where y denotes the label for the given example x. \n",
    "\n",
    "Instead of a binary classification, i.e. real or fake, the SGAN Discriminator's goal is to correctly categorize the inout example into its corresponding class if the example is real, or reject the example as fake (which can be thought of as a special additional class).\n",
    "\n",
    "## Training process\n",
    "In a regukar GAN, we train the Discriminator by computing the loss for D(x) and D(x*) and backpropagating the totla loss to update the Discriminaor's trainable parameters to minimize the loss. The Generator is trained by back propagating the Discriminator's loss for D(x*), seeking to maximise it, so that the fake examples it synthesises are msiclassified as real.\n",
    "\n",
    "To train the SGAN, in addition to D(x) and D(x*), we also have to compute the loss for the supervised training examples: D((x, y)). These losses correspond to the dual learning objective trhe SGAN has to grapple with: distinguishing between real examples from the fake ones while also learning to classify real examples to their correct classes. These dual objectives correspond to two kinds of losses: the supervised loss and the unsupervised loss.\n",
    "\n",
    "## Training objective\n",
    "All previous GAN have the main prepose for the Generator to produce realistic fakes. In SGAN we primarily care about the discriminator. The goal of the process is to make this network into a semi-supervised classifier whose accuracy is as close to a fully supervised classifier. The generator's goal is to learn the relevant patterns in the data, enhancing its classification accuracy.\n",
    "\n",
    "## Implementation\n",
    "To solve the multiclass classification problem of distinguishing between the real labels, the discriminator uses the softmax function, which gives probability distribution over a specific number of classes. The higher the probabiltity assigned to a given label, the more confident the Discriminator is that the example belongs to the given class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.0\n",
      "  latest version: 4.10.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/a/anaconda3/envs/new-2d-dcgan-env\n",
      "\n",
      "  added / updated specs:\n",
      "    - keras\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    pyyaml-5.3.1               |   py38h8df0ef7_1         190 KB  conda-forge\n",
      "    yaml-0.2.5                 |       h516909a_0          82 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         273 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  keras              conda-forge/noarch::keras-2.4.3-py_0\n",
      "  pyyaml             conda-forge/linux-64::pyyaml-5.3.1-py38h8df0ef7_1\n",
      "  yaml               conda-forge/linux-64::yaml-0.2.5-h516909a_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "yaml-0.2.5           | 82 KB     | ##################################### | 100% \n",
      "pyyaml-5.3.1         | 190 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import (Activation, BatchNormalization, Concatenate, Dense,\n",
    "                          Dropout, Flatten, Input, Lambda, Reshape)\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model input dimensions\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "z_dim = 100\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n",
    "Only uses a fraction of the labels from the dataset and disregards the rest of the labels. This is accomplished by sampling only from the first num_sampled images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class Dataset:\n",
    "    def __init__(self, num_labeled):\n",
    "\n",
    "        # Number labeled examples to use for training\n",
    "        self.num_labeled = num_labeled\n",
    "\n",
    "        # Load the MNIST dataset\n",
    "        (self.x_train, self.y_train), (self.x_test,\n",
    "                                       self.y_test) = mnist.load_data()\n",
    "\n",
    "        def preprocess_imgs(x):\n",
    "            # Rescale [0, 255] grayscale pixel values to [-1, 1]\n",
    "            x = (x.astype(np.float32) - 127.5) / 127.5\n",
    "            # Expand image dimensions to width x height x channels\n",
    "            x = np.expand_dims(x, axis=3)\n",
    "            return x\n",
    "\n",
    "        def preprocess_labels(y):\n",
    "            return y.reshape(-1, 1)\n",
    "\n",
    "        # Training data\n",
    "        self.x_train = preprocess_imgs(self.x_train)\n",
    "        self.y_train = preprocess_labels(self.y_train)\n",
    "\n",
    "        # Testing data\n",
    "        self.x_test = preprocess_imgs(self.x_test)\n",
    "        self.y_test = preprocess_labels(self.y_test)\n",
    "\n",
    "    def batch_labeled(self, batch_size):\n",
    "        # Get a random batch of labeled images and their labels\n",
    "        idx = np.random.randint(0, self.num_labeled, batch_size)\n",
    "        imgs = self.x_train[idx]\n",
    "        labels = self.y_train[idx]\n",
    "        return imgs, labels\n",
    "\n",
    "    def batch_unlabeled(self, batch_size):\n",
    "        # Get a random batch of unlabeled images\n",
    "        idx = np.random.randint(self.num_labeled, self.x_train.shape[0],\n",
    "                                batch_size)\n",
    "        imgs = self.x_train[idx]\n",
    "        return imgs\n",
    "\n",
    "    def training_set(self):\n",
    "        x_train = self.x_train[range(self.num_labeled)]\n",
    "        y_train = self.y_train[range(self.num_labeled)]\n",
    "        return x_train, y_train\n",
    "\n",
    "    def test_set(self):\n",
    "        return self.x_test, self.y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "num_labeled = 100\n",
    "\n",
    "dataset = Dataset(num_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator - same as DCGAN\n",
    "def build_generator(z_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Reshape input into 7x7x256 tensor via a fully connected layer\n",
    "    model.add(Dense(256 * 7 * 7, input_dim=z_dim))\n",
    "    model.add(Reshape((7, 7, 256)))\n",
    "\n",
    "    # Transposed convolution layer, from 7x7x256 into 14x14x128 tensor\n",
    "    model.add(Conv2DTranspose(128, kernel_size=3, strides=2, padding='same'))\n",
    "\n",
    "    # Batch normalization\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Leaky ReLU activation\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    # Transposed convolution layer, from 14x14x128 to 14x14x64 tensor\n",
    "    model.add(Conv2DTranspose(64, kernel_size=3, strides=1, padding='same'))\n",
    "\n",
    "    # Batch normalization\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Leaky ReLU activation\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    # Transposed convolution layer, from 14x14x64 to 28x28x1 tensor\n",
    "    model.add(Conv2DTranspose(1, kernel_size=3, strides=2, padding='same'))\n",
    "\n",
    "    # Output layer with tanh activation\n",
    "    model.add(Activation('tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "The discriminator has dual objective\n",
    "- Distinguish real examples from fake examples. For this, SGAN Discriminator uses the sigmoid function, outputting the a single output probability for binary classification.\n",
    "- For the real examples, accurately classify their label. For this, the SGAN Discriminator uses the softmax function, outputting a vector of probabilities, one for each of the target classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core discriminator network\n",
    "def build_discriminator_net(img_shape):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolutional layer, from 28x28x1 into 14x14x32 tensor\n",
    "    model.add(\n",
    "        Conv2D(32,\n",
    "               kernel_size=3,\n",
    "               strides=2,\n",
    "               input_shape=img_shape,\n",
    "               padding='same'))\n",
    "\n",
    "    # Leaky ReLU activation\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    # Convolutional layer, from 14x14x32 into 7x7x64 tensor\n",
    "    model.add(\n",
    "        Conv2D(64,\n",
    "               kernel_size=3,\n",
    "               strides=2,\n",
    "               input_shape=img_shape,\n",
    "               padding='same'))\n",
    "\n",
    "    # Batch normalization\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Leaky ReLU activation\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    # Convolutional layer, from 7x7x64 tensor into 3x3x128 tensor\n",
    "    model.add(\n",
    "        Conv2D(128,\n",
    "               kernel_size=3,\n",
    "               strides=2,\n",
    "               input_shape=img_shape,\n",
    "               padding='same'))\n",
    "\n",
    "    # Batch normalization\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Leaky ReLU activation\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "    # Droupout\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Flatten the tensor\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully connected layer with num_classes neurons\n",
    "    model.add(Dense(num_classes))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Discriminator - gives probability of what class the image belongs to\n",
    "def build_discriminator_supervised(discriminator_net):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(discriminator_net)\n",
    "\n",
    "    # Softmax activation, giving predicted probability distribution over the real classes\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Discriminator - gives probability of whether real or fake\n",
    "def build_discriminator_unsupervised(discriminator_net):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(discriminator_net)\n",
    "\n",
    "    def predict(x):\n",
    "        # Transform distribution over real classes into a binary real-vs-fake probability\n",
    "        prediction = 1.0 - (1.0 /\n",
    "                            (K.sum(K.exp(x), axis=-1, keepdims=True) + 1.0))\n",
    "        return prediction\n",
    "\n",
    "    # 'Real-vs-fake' output neuron defined above\n",
    "    model.add(Lambda(predict))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN model\n",
    "def build_gan(generator, discriminator):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Combined Generator -> Discriminator model\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Discriminator network:\n",
    "# These layers are shared during supervised and unsupervised training\n",
    "discriminator_net = build_discriminator_net(img_shape)\n",
    "\n",
    "# Build & compile the Discriminator for supervised training\n",
    "discriminator_supervised = build_discriminator_supervised(discriminator_net)\n",
    "discriminator_supervised.compile(loss='categorical_crossentropy',\n",
    "                                 metrics=['accuracy'],\n",
    "                                 optimizer=Adam())\n",
    "\n",
    "# Build & compile the Discriminator for unsupervised training\n",
    "discriminator_unsupervised = build_discriminator_unsupervised(discriminator_net)\n",
    "discriminator_unsupervised.compile(loss='binary_crossentropy',\n",
    "                                   optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build GAN model\n",
    "generator = build_generator(z_dim)\n",
    "\n",
    "# Keep Discriminator’s parameters constant for Generator training\n",
    "discriminator_unsupervised.trainable = False\n",
    "\n",
    "# Build and compile GAN model with fixed Discriminator to train the Generator\n",
    "# Note that we are using the Discriminator version with unsupervised output\n",
    "gan = build_gan(generator, discriminator_unsupervised)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "supervised_losses = []\n",
    "iteration_checkpoints = []\n",
    "\n",
    "\n",
    "def train(iterations, batch_size, sample_interval):\n",
    "\n",
    "    # Labels for real images: all ones\n",
    "    real = np.ones((batch_size, 1))\n",
    "\n",
    "    # Labels for fake images: all zeros\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        # -------------------------\n",
    "        #  Train the Discriminator\n",
    "        # -------------------------\n",
    "\n",
    "        # Get labeled examples\n",
    "        imgs, labels = dataset.batch_labeled(batch_size)\n",
    "\n",
    "        # One-hot encode labels\n",
    "        labels = to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "        # Get unlabeled examples\n",
    "        imgs_unlabeled = dataset.batch_unlabeled(batch_size)\n",
    "\n",
    "        # Generate a batch of fake images\n",
    "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "        gen_imgs = generator.predict(z)\n",
    "\n",
    "        # Train on real labeled examples\n",
    "        d_loss_supervised, accuracy = discriminator_supervised.train_on_batch(imgs, labels)\n",
    "\n",
    "        # Train on real unlabeled examples\n",
    "        d_loss_real = discriminator_unsupervised.train_on_batch(\n",
    "            imgs_unlabeled, real)\n",
    "\n",
    "        # Train on fake examples\n",
    "        d_loss_fake = discriminator_unsupervised.train_on_batch(gen_imgs, fake)\n",
    "\n",
    "        d_loss_unsupervised = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train the Generator\n",
    "        # ---------------------\n",
    "\n",
    "        # Generate a batch of fake images\n",
    "        z = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "        gen_imgs = generator.predict(z)\n",
    "\n",
    "        # Train Generator\n",
    "        g_loss = gan.train_on_batch(z, np.ones((batch_size, 1)))\n",
    "\n",
    "        if (iteration + 1) % sample_interval == 0:\n",
    "\n",
    "            # Save Discriminator supervised classification loss to be plotted after training\n",
    "            supervised_losses.append(d_loss_supervised)\n",
    "            iteration_checkpoints.append(iteration + 1)\n",
    "\n",
    "            # Output training progress\n",
    "            print(\n",
    "                \"%d [D loss supervised: %.4f, acc.: %.2f%%] [D loss unsupervised: %.4f] [G loss: %f]\"\n",
    "                % (iteration + 1, d_loss_supervised, 100 * accuracy,\n",
    "                   d_loss_unsupervised, g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "iterations = 8000\n",
    "batch_size = 32\n",
    "sample_interval = 800\n",
    "\n",
    "# Train the SGAN for the specified number of iterations\n",
    "train(iterations, batch_size, sample_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Discriminator supervised loss\n",
    "losses = np.array(supervised_losses)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(iteration_checkpoints, losses, label=\"Discriminator loss\")\n",
    "\n",
    "plt.xticks(iteration_checkpoints, rotation=90)\n",
    "\n",
    "plt.title(\"Discriminator – Supervised Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset.training_set()\n",
    "y = to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "# Compute classification accuracy on the training set\n",
    "_, accuracy = discriminator_supervised.evaluate(x, y)\n",
    "print(\"Training Accuracy: %.2f%%\" % (100 * accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dataset.test_set()\n",
    "y = to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "# Compute classification accuracy on the test set\n",
    "_, accuracy = discriminator_supervised.evaluate(x, y)\n",
    "print(\"Test Accuracy: %.2f%%\" % (100 * accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
